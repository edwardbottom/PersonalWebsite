---
layout: post
title:  "Assesing the Biological Validity of Neural Networks"
excerpt: "A paper that discusses weather the successes of neural networks make them a biologically realistic model of the human mind.  "
categories: jekyll update
---

The brain is something of awe and wonder: a network of millions of small neurons capable of all human achievement.Naturally, cognitive 
scientists have made many attempts to imitate the human brain’s function. One of the most promising attempts is the neural network, which
builds increasingly complex networks using simple units like the connectionist networks of neuronsin the brain. Neural networks present a 
biologically plausible means of modeling the human brain in that they manipulate simple Boolean input through a layered network of
connections. Although successful in its implementation to perform complex tasks, one could argue that neural networks are too simplistic
in their approach to modeling the brain’s mechanics and are not biologically accurate as they are overly parallel, require extensive 
training, and require feedback from an outside source. However, these arguments focus primarily on the intentionally simplified structure 
of the neural network and not the functions that it tries to model. In this paper, I argue the increasing connectionist based size of 
neural networks along with local learning algorithms that require limited feedback make neural networks biologically valid models of 
cognition. 


The neurons the neural networks seek to model are simple cells in the brain that make up massively modular systems. A single neuron is 
comprised of a cell body (soma) containing the nucleus, and root like extensions called dendrites (Bermudez 212). The soma then connects
to the axon which terminates at an endbulb that connects with the dendrites of other nearby neurons through a connection known as the 
synapse (213). The synapse forms the basis of connections between neurons and allows for the flow of information across neurons. Although
seemingly insignificant, a single neuron can receive inputs from 10,000 to 50,000 neurons by firing an electrical pulse along the axon and
sending it to another neuron when a certain threshold is reached. 

The general premise of neural networks is that they model “the most fundamental feature of the human brain, connectivity” (221). Neural 
networks seek to emulate the transfer of information across the synapse of thousands of neurons, but ignore the transfer of energy within 
neurons from the axon to the dendrites. Neural networks focus more on how the brain functions and not the specialization of neurons in the
brain. Rather, they focus on how the “patterns of activation across populations of neurons give rise to perception, memory, and high-level
cognition” (211). 

Boolean logic deals with statements that are either true or false and at the basic level are quite simple given only two inputs and two 
outputs exists. Boolean operators include operations on Boolean values such as AND, OR, XOR and NOT. Each one returns its own respective
value based on its input and can be modeled in a truth table. For example, AND always returns false unless both of its input values are 
true, OR returns true if any of its inputs are true and false otherwise, XOR returns true if only one of its inputs is true and false
otherwise and NOT turns true into false and false into true. Neural networks seek to emulate the connectivity of the neurons described
previously through single units and logic gates that gradually manipulate information through Boolean logic. A single unit “represents 
some very basic Boolean function” and by stringing them together complex networks that mimic human thought can be created (219). The 
creation of massive networks using single Boolean values and operations is the foundation of artificial neural networks. 

However, networks in the brain are not stationary and change, adapt, and learn from stimuli and experiences. Computational cognitive 
scientists have developed two forms of learning for neural networks: Hebbian learning and perceptron convergence. Hebbian learning “is a
bottom up associative process” that learns through “synaptic modification” (220). It looks for general trends and common patterns in 
networks and requires no feedback, an unsupervised process summarized by the phrase “neurons that fire together wire together” (221).
In contrast, perceptron convergence requires feedback and is based on what happens locally and seeks to decrease the discrepancy between
the actual output and the desired output (221). These two means of learning are a source of both biological invalidity for neural network
s in that perceptron convergence requires feedback from an outside source unlike the brain, and validity in that Hebbian learning can
function independent of an outside source like the brain. 

Neural networks are without a doubt a very successful way of articulating complex ideas: this is evident in its successes in modeling 
the physical reasoning of children (261) and in modeling language learning (245). The source of debate is how well the models mimic the
activity that goes on in the mind; if they are biologically plausible models of cognition. 

One major counter argument is that neural networks are comprised of simplified “all homogenous” units while “there are many different 
types of neurons in the brain” (230). This distinction makes it appear as though there is more that goes on in the transfer on energy 
between neurons besides simply manipulating Boolean values. Neurons, although small are still none the less complex cells with variation
between their location and distinct modularity and functionality. This distinction suggests that there is a difference between each 
neuron and that their functions and signals vary based on their location and structure. Boolean logic may be too oversimplified to 
account for differences between neurons as it only allows for two outputs and two inputs and a finite number of operations. 

Both brains and neural networks transmit many signals at the same time using parallel processing. However, brains and neural networks
transfer information differently in that “brains are nowhere near as massively parallel as neural networks” (230). Due to the complexity
of the neurons compared to the neural networks, brains make significantly more connections than neural networks, as many as 200,000 
neurons compared to a mere 5,000 neurons (230). One could argue that this distinction exists simply because neural networks do not exist
on the scale that the human brain does. Most networks exist to perform a single task like playing a game of Go or language processing 
and not to sustain human life. However, larger neural networks become increasing parallel due to the simplicity of the units and start
to look and act less like the human brain. 

The extreme specificity of the input into the neural network and the need for supervision while learning is another point of distinction
in the biological plausibility of neural networks. Neural networks take significantly longer to train than most biological neural pathway
s. This is in part because perceptron convergence requires neural networks start with a nearly random assortment of weights and
distributions (231). They have no baseline knowledge or understanding of how the network is supposed to function. Rather, they take in
very specific inputs and receive human feedback based on the outcome. In the case of the brain learning, people tend to start with a 
baseline understanding of what is supposed to happen and tend to pick up information at a much faster rate than neural networks with 
significantly less feedback and supervision. 

However, these distinctions disregard the connectionist goal of neural networks and their intentionally simplified structure. One of 
the major points against neural networks is the over simplification of the units that make up the neural network, but such an argument 
ignores the fundamental idea of neural networks: to imitate the connectivity of the brain. As stated by Bermudez, “functional maps of 
the brain tell us very little about how the brain carries out functions”, and that the focus of the neural nets is to model the major 
functions of the brain, not the exact structure and details of the individual neurons in the brain (210).  

Although perceptron convergence requires feedback from the user and contrasts how neurons learn, cognitive scientists have developed 
“local algorithms” like Hebbian learning in which “an individual unit’s weight changes directly as a function of the inputs and outputs
to a unit” (231). These “competitive networks” “do not require any feedback” and “have no external teacher or fixed output” (231). They
are much more like the human brain and are a significantly more biologically plausible model of the human mind than neural networks that
use perceptron convergence as they can act independently of external stimuli and learn and develop without supervision much like the 
human brain. 

Another major argument against neural networks is the discrepancy in the size of neural networks compared to the size of the network of
neurons in the brain. The largest neural networks are a fraction of the size of networks in the brain (230), yet perform similar tasks.
This discrepancy in size would suggest that the connectionist network of the brain is vastly different and significantly more complex 
and sophisticated than the much smaller neural networks. However, this distinction could stem from reasons other than the structure 
alone. More likely, it comes from the more numerous applications and processes going on in the human brain. Neural networks are by 
design overly simplified to perform very specific tasks with simple Boolean logic and do not need the extra complexity to perform all 
the functions in the brain required to sustain human life. 

The parallel structure of the neural networks is in alignment with the cognitive model of lexical processing that holds that lexical 
processing in the brain is parallel and not serial (78). Artificial networks are more parallel than the connections of biological
neurons, but it is also worth noting that all parallel processes are inherently somewhat serial. The brain operates in parallel so that 
it can use multiple networks at the same time whereas serial processing only allows for one process at a time. Likewise, the brain is
more parallel than serial and thus it is more like neural networks in its transfer of information than it is different. The overly 
parallel nature of neural networks more likely stems from focusing on the connections and ignoring the serial transfer of information
within neurons than it does with differences in the foundations of the two networks. 

Neural networks on the scale and scope of the human mind have not been created yet, but as their functions become more generalized to
humans and their sizes increase, the networks may begin to look and function more like that of the human mind. The neural networks that
currently exist are mere fractions of that of the human mind, 5,000 connections compared to 200,000 connections (230). As technology 
develops and networks begin to operate together using local algorithms to perform many unsupervised tasks, neural networks are becoming
increasingly more biologically valid and more like the human brain than different. 

Work Cited


BermuÌdez, JoseÌ Luis. Cognitive Science: An Introduction to the Science of the Mind. Cambridge University Press, 2014.